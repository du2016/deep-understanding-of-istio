func (s *Server) initClusterRegistries(args *PilotArgs) (err error) {
	if hasKubeRegistry(args.RegistryOptions.Registries) {
		log.Info("initializing Kubernetes cluster registry")
		mc, err := controller.NewMulticluster(s.kubeClient,
			args.RegistryOptions.ClusterRegistriesNamespace,
			args.RegistryOptions.KubeOptions,
			s.ServiceController(),
			s.XDSServer,
			s.environment)

		if err != nil {
			log.Info("Unable to create new Multicluster object")
			return err
		}

		s.multicluster = mc
	}
	return nil
}



func (m *Multicluster) initSecretController(kc kubernetes.Interface) {
	m.secretController = secretcontroller.StartSecretController(kc,
		m.AddMemberCluster,
		m.UpdateMemberCluster,
		m.DeleteMemberCluster,
		m.secretNamespace,
		m.syncInterval)
}


func StartSecretController(k8s kubernetes.Interface, addCallback addSecretCallback,
	updateCallback updateSecretCallback, removeCallback removeSecretCallback, namespace string, syncInterval time.Duration) *Controller {
	stopCh := make(chan struct{})
	clusterStore := newClustersStore()
	// 新的controller配置
	controller := NewController(k8s, namespace, clusterStore, addCallback, updateCallback, removeCallback)
	controller.syncInterval = syncInterval

	go controller.Run(stopCh)

	return controller
}



func (c *Controller) processNextItem() bool {
	secretName, quit := c.queue.Get()
	if quit {
		return false
	}
	defer c.queue.Done(secretName)

	err := c.processItem(secretName.(string))
	if err == nil {
		// No error, reset the ratelimit counters
		c.queue.Forget(secretName)
	} else if c.queue.NumRequeues(secretName) < maxRetries {
		log.Errorf("Error processing %s (will retry): %v", secretName, err)
		c.queue.AddRateLimited(secretName)
	} else {
		log.Errorf("Error processing %s (giving up): %v", secretName, err)
		c.queue.Forget(secretName)
		utilruntime.HandleError(err)
	}

	return true
}


func (c *Controller) processItem(secretName string) error {
	if secretName == initialSyncSignal {
		c.mu.Lock()
		c.initialSync = true
		c.mu.Unlock()
		return nil
	}

	obj, exists, err := c.informer.GetIndexer().GetByKey(secretName)
	if err != nil {
		return fmt.Errorf("error fetching object %s error: %v", secretName, err)
	}

	if exists {
		c.addMemberCluster(secretName, obj.(*corev1.Secret))
	} else {
		c.deleteMemberCluster(secretName)
	}

	return nil
}


addMemberCluster
调用addcallback和updatecallback


func (m *Multicluster) AddMemberCluster(clients kubelib.Client, clusterID string) error {
	// stopCh to stop controller created here when cluster removed.
	stopCh := make(chan struct{})
	var remoteKubeController kubeController
	remoteKubeController.stopCh = stopCh
	m.m.Lock()
	options := Options{
		SystemNamespace:   m.systemNamespace,
		WatchedNamespaces: m.WatchedNamespaces,
		ResyncPeriod:      m.ResyncPeriod,
		DomainSuffix:      m.DomainSuffix,
		XDSUpdater:        m.XDSUpdater,
		ClusterID:         clusterID,
		NetworksWatcher:   m.networksWatcher,
		Metrics:           m.metrics,
		EndpointMode:      m.endpointMode,
		SyncInterval:      m.syncInterval,
	}
	log.Infof("Initializing Kubernetes service registry %q", options.ClusterID)
	kubectl := NewController(clients, options)

	remoteKubeController.Controller = kubectl
	m.serviceController.AddRegistry(kubectl)

	m.remoteKubeControllers[clusterID] = &remoteKubeController
	m.m.Unlock()

	// Only need to add service handler for kubernetes registry as `initRegistryEventHandlers`,
	// because when endpoints update `XDSUpdater.EDSUpdate` has already been called.
	_ = kubectl.AppendServiceHandler(func(svc *model.Service, ev model.Event) { m.updateHandler(svc) })

	go kubectl.Run(stopCh)
	webhookConfigName := strings.ReplaceAll(validationWebhookConfigNameTemplate, validationWebhookConfigNameTemplateVar, m.secretNamespace)
	if m.fetchCaRoot != nil {
		nc := NewNamespaceController(m.fetchCaRoot, clients)
		go nc.Run(stopCh)
		go webhooks.PatchCertLoop(features.InjectionWebhookConfigName.Get(), webhookName, m.caBundlePath, clients.Kube(), stopCh)
		valicationWebhookController := webhooks.CreateValidationWebhookController(clients, webhookConfigName,
			m.secretNamespace, m.caBundlePath, true)
		if valicationWebhookController != nil {
			go valicationWebhookController.Start(stopCh)
		}
	}

	clients.RunAndWait(stopCh)
	return nil
}

调用NewController


func NewController(kubeClient kubelib.Client, options Options) *Controller {
	// The queue requires a time duration for a retry delay after a handler error
	c := &Controller{
		domainSuffix:                options.DomainSuffix,
		client:                      kubeClient.Kube(),
		queue:                       queue.NewQueue(1 * time.Second),
		clusterID:                   options.ClusterID,
		xdsUpdater:                  options.XDSUpdater,
		servicesMap:                 make(map[host.Name]*model.Service),
		nodeSelectorsForServices:    make(map[host.Name]labels.Instance),
		nodeInfoMap:                 make(map[string]kubernetesNode),
		externalNameSvcInstanceMap:  make(map[host.Name][]*model.ServiceInstance),
		workloadInstancesByIP:       make(map[string]*model.WorkloadInstance),
		workloadInstancesIPsByName:  make(map[string]string),
		registryServiceNameGateways: make(map[host.Name]uint32),
		networkGateways:             make(map[host.Name]map[string][]*model.Gateway),
		networksWatcher:             options.NetworksWatcher,
		metrics:                     options.Metrics,
		syncInterval:                options.GetSyncInterval(),
	}

	if options.SystemNamespace != "" {
		c.nsInformer = informers.NewSharedInformerFactoryWithOptions(c.client, options.ResyncPeriod,
			informers.WithTweakListOptions(func(listOpts *metav1.ListOptions) {
				listOpts.FieldSelector = fields.OneTermEqualSelector("metadata.name", options.SystemNamespace).String()
			})).Core().V1().Namespaces().Informer()
		registerHandlers(c.nsInformer, c.queue, "Namespaces", c.onNamespaceEvent, nil)
	}

	c.serviceInformer = kubeClient.KubeInformer().Core().V1().Services().Informer()
	c.serviceLister = kubeClient.KubeInformer().Core().V1().Services().Lister()
	registerHandlers(c.serviceInformer, c.queue, "Services", c.onServiceEvent, nil)

	switch options.EndpointMode {
	case EndpointsOnly:
		c.endpoints = newEndpointsController(c, kubeClient.KubeInformer().Core().V1().Endpoints())
	case EndpointSliceOnly:
		c.endpoints = newEndpointSliceController(c, kubeClient.KubeInformer().Discovery().V1beta1().EndpointSlices())
	}

	// This is for getting the node IPs of a selected set of nodes
	c.nodeInformer = kubeClient.KubeInformer().Core().V1().Nodes().Informer()
	c.nodeLister = kubeClient.KubeInformer().Core().V1().Nodes().Lister()
	registerHandlers(c.nodeInformer, c.queue, "Nodes", c.onNodeEvent, nil)

	c.pods = newPodCache(c, kubeClient.KubeInformer().Core().V1().Pods(), func(key string) {
		item, exists, err := c.endpoints.getInformer().GetStore().GetByKey(key)
		if err != nil {
			log.Debugf("Endpoint %v lookup failed with error %v, skipping stale endpoint", key, err)
			return
		}
		if !exists {
			log.Debugf("Endpoint %v not found, skipping stale endpoint", key)
			return
		}
		c.queue.Push(func() error {
			return c.endpoints.onEvent(item, model.EventUpdate)
		})
	})
	registerHandlers(c.pods.informer, c.queue, "Pods", c.pods.onEvent, nil)

	return c
}

完成远程集群的监听